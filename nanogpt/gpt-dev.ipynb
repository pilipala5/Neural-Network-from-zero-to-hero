{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件下载成功\n"
     ]
    }
   ],
   "source": [
    "# 下载需要的数据集\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "with open('../dataset/input.txt', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print('文件下载成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('length of dataset in characters: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "字符数目:  65\n"
     ]
    }
   ],
   "source": [
    "# 获取所有的字符\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('字符数目: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# 创建一个字符级别的分词器\n",
    "stoi = { ch: i for i, ch in enumerate(chars)}\n",
    "itos = { i: ch for ch, i in stoi.items()}\n",
    "encoder = lambda s: [stoi[ch] for ch in s]\n",
    "decoder = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encoder(\"hii there\"))\n",
    "print(decoder(encoder(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.dtype, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据划分为训练集和测试集\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([18]) -> target: 47\n",
      "context: tensor([18, 47]) -> target: 56\n",
      "context: tensor([18, 47, 56]) -> target: 57\n",
      "context: tensor([18, 47, 56, 57]) -> target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]) -> target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]) -> target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]) -> target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> target: 58\n"
     ]
    }
   ],
   "source": [
    "# 创建样本与标签\n",
    "block_size = 8\n",
    "train_data = data[:block_size + 1]\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "for t in range(0, block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"context: {context} -> target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "----------------------------------------\n",
      "context: tensor([18]), target: 47\n",
      "context: tensor([18, 47]), target: 56\n",
      "context: tensor([18, 47, 56]), target: 57\n",
      "context: tensor([18, 47, 56, 57]), target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n",
      "context: tensor([18]), target: 47\n",
      "context: tensor([18, 47]), target: 56\n",
      "context: tensor([18, 47, 56]), target: 57\n",
      "context: tensor([18, 47, 56, 57]), target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n",
      "context: tensor([18]), target: 47\n",
      "context: tensor([18, 47]), target: 56\n",
      "context: tensor([18, 47, 56]), target: 57\n",
      "context: tensor([18, 47, 56, 57]), target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n",
      "context: tensor([18]), target: 47\n",
      "context: tensor([18, 47]), target: 56\n",
      "context: tensor([18, 47, 56]), target: 57\n",
      "context: tensor([18, 47, 56, 57]), target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    获取一个批次的训练数据, 其样本和标签大小皆为 (4, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "print('-'*40)\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, :j+1]\n",
    "        target = yb[i, j]\n",
    "        print(f\"context: {context}, target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.4032, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# 创建一个Bygram模型，只用一个字符进行模型的生成\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, target=None):\n",
    "        \"\"\"\n",
    "        模型的前向传播过程\n",
    "\n",
    "        param idx: 样本，形状为(batch_size, block_size)\n",
    "        param target: 标签， 形状为(batch_size, block_size)\n",
    "        return logits: 得分\n",
    "        return loss: 损失\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, block_size, vocab_size)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        生成语句\n",
    "\n",
    "        param idx: 开始的索引\n",
    "        param max_token_number: 生成的词元数目\n",
    "        return idx: 增长的序列\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decoder(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.173467755317688\n"
     ]
    }
   ],
   "source": [
    "# 训练bigram模型\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)\n",
    "for _ in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zX!p&\n",
      "\n",
      ":&ZBirstoit$I!qf3kGN!UxEstt Cir&DnpcU?q3SPlF$N CD.I,SZ?q-UQxfrstKpv&Cp$lviqVTb?RCaq CXaH.AVJF.W?lSPSPT'ehszkJyl\n",
      "oTyh$VmgCGbV&HAVrs&azreQ?-on.yCMrx;w?iWo'K!P!ujMLv&!a&vCstRnlaHZLNvhoEI&dZp.,;KmyeSS3Qqoiru33:dbnrevU?up CTERFZybcFirBehz CJFbPWHe.jag\n",
      "pUhSAXmstxOENstukthFD&'jFAkN!AkHrC-uvyh$wArslg\n",
      "r.?dEBclxxlyNs CirsrsSP&Z?SVOK$-!$YYCVusu3dlvv&MBcUEVuzlr'DFiW:CjxmZKivpQ!M;Fq-KuQqHA:wWAJ33OsJq-T'adZ:l!gZ:qfRj?UQNT'XcL-uqtZO'e:'XASiFEa CttSP C CXNjGJhyI&DMliraZ!ctR\n",
      "wxHWWgYCjKuCjKmV.-KIOLK$'no,p$pU?gQ&Nw!KuUt COghckR?RFuj3QHOsEJRJGJFiv CHYwGNstgClgJ&yFEX$R\n",
      "pluxpttIRlI&uQ?BL!UfenEQ?YQkgX&lyRgI!?u3:CbM?OicVj3O'AUCPRw'nmUG\n",
      "Eb$!WIg\n",
      "oBq$hSCO,iq-wDqM$I,WwZIVK:CjK$CqSVkQhkz C?ERycITqhUGlavrdX'qxqfx?Uir;'Q!LQbcPTJbHd!v C CtRgzkcYX'xla CittD3QL$zkH;GerrW$s-oiGvCep C CJ\n",
      "RrsuZyDb?wOPZgYCwwN CirJHxHUcP!a-wGXEaeCagbev$pc'VKBS$VjX:CG,SPG; CbZEREjWgCitgO Cjb?oireQKp,j\n",
      "wLEs Cir'VbL!o.st Cx:Zxja!ztg'O'&yiYgjWWdGJuq-wz,eRL;A CEyFBjuWW!irnUi-s3&QWcPynqfRuloikbOIXRCKU'zeSKSx;l;k-SjirnFi!zFb\n",
      "oC&eZOWCjjs-qf \n"
     ]
    }
   ],
   "source": [
    "print(decoder(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意力机制 self_attention\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# 单头注意力机制\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size=16)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "wei = q @ k.transpose(-2, -1)   # (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v   \n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
