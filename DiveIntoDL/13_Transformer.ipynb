{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:44:08.802092Z",
     "start_time": "2024-08-24T09:44:08.798281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4a6353e8a97f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:55.997514Z",
     "start_time": "2024-08-24T09:21:55.992044Z"
    }
   },
   "outputs": [],
   "source": [
    "# positionalEncoding is a fixed mat, only add 2 matrix in forward pass\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.encoding = torch.zeros([max_len, d_model], requires_grad=False)    # (max_len, d_model)\n",
    "        \n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)   # (max_len, 1)\n",
    "        _2i = torch.arange(0, d_model, 2)    # (d_model/2, )\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/ 10000 ** (2i/d_model)) ; PE(pos, 2i+1) = cos(pos/ 10000 ** (2i/d_model))\n",
    "        # broadcast in this way:\n",
    "        # 10000 ** (_2i / d_model) -> (d_model/2, ) -> (1, d_model/2) -> (max_len, d_model/2)             \n",
    "        # pos -> (max_len, 1) -> (max_len, d_model/2)\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))    \n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):   # (batch_size, max_len, d_model)  \n",
    "        return x + self.encoding    # (batch_size, max_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d5404c0f873008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.003149Z",
     "start_time": "2024-08-24T09:21:55.998523Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):   # (batch, max_len)\n",
    "        x = self.tok_emb(x)\n",
    "        x = self.pos_emb(x)\n",
    "        return self.dropout(x)  # (batch_ max_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81039faa8a0fa19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.008825Z",
     "start_time": "2024-08-24T09:21:56.004160Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # (batch_size, h, max_len, dk)\n",
    "        # Attention(Q,K,V) = softmax(Q @ K_T / sqrt(dk)) @ V\n",
    "        _, _, _, dk = k.shape\n",
    "        \n",
    "        k_T = k.transpose(2, 3) # (batch_size, h, dk, max_len)\n",
    "        scores = torch.matmul(q, k_T) / math.sqrt(dk)  # (batch_size, h, max_len, max_len)\n",
    "        \n",
    "        if mask is not None:    # (batch_size, 1, 1, max_len) or (batch_size, 1, max_len, 1)\n",
    "            scores = scores.masked_fill(mask == 0, -10000)\n",
    "        \n",
    "        scores = self.softmax(scores)  # (batch_size, h, max_len, max_len)\n",
    "        \n",
    "        outputs = torch.matmul(scores, v)   # (batch_size, h, max_len, dv)\n",
    "        return outputs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a89230ed728324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.016627Z",
     "start_time": "2024-08-24T09:21:56.010960Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"d_model is not divisible by n_head\"\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # shapes of q,k,v are all (batch_size, max_len, d_model)\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v) # shapes do not change\n",
    "        \n",
    "        batch_size, max_len, d_model = k.shape\n",
    "        q = q.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)    # (batch_size, n_head, max_len, d_tensor)  where d_tensor = d_model / n_head\n",
    "        k = k.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)    \n",
    "        v = v.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n",
    "        \n",
    "        outputs, scores = self.attention(q, k, v, mask=mask)    # (batch_size, n_head, max_len, d_tensor), (batch_size, n_head, max_len, max_len)\n",
    "        \n",
    "        # concat\n",
    "        outputs = outputs.transpose(1, 2)\n",
    "        outputs = outputs.contiguous().view(batch_size, max_len, d_model)       # (batch_size, max_len, d_model)\n",
    "        \n",
    "        # Linear\n",
    "        outputs = self.w_o(outputs) # (batch_size, max_len, d_model)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2477c7f1e9a8b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.023921Z",
     "start_time": "2024-08-24T09:21:56.017632Z"
    }
   },
   "outputs": [],
   "source": [
    "# layer norm (bn in dim -1)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super().__init__()\n",
    "        self.gama = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):   # (batch_size, max_len, d_model)\n",
    "        miu = torch.mean(x, dim=-1, keepdim=True)   # (batch_size, max_len, 1)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)    # in paper, used biased var\n",
    "        \n",
    "        x_bar = (x - miu) / torch.sqrt(var + self.eps)  # (batch_size, max_len, d_model)\n",
    "        return self.gama * x_bar + self.beta    # (batch_size, max_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af1550aa46b59b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.029709Z",
     "start_time": "2024-08-24T09:21:56.024931Z"
    }
   },
   "outputs": [],
   "source": [
    "# position wise feed forward\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(n_hidden, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):   # (batch_size, max_len, d_model)\n",
    "        x = self.linear_1(x) # (batch_size, max_len, n_hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x) # (batch_size, max_len, n_hidden)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e865639de7c4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.035907Z",
     "start_time": "2024-08-24T09:21:56.030718Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, n_head, drop_prob):\n",
    "        super().__init__()\n",
    "        # sublayer 1\n",
    "        self.attention = MultiHeadAttention(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        # sublayer 2\n",
    "        self.ffn = PositionWiseFeedForward(d_model, n_hidden, drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x, src_mask):   # (batch_size, max_len, n_hidden)  all block will not change the shape of tensor\n",
    "        # sublayer 1\n",
    "        _x = x\n",
    "        x = self.attention(x, x, x, mask=src_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # sublayer2 \n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becbfc7b8e731eab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.041999Z",
     "start_time": "2024-08-24T09:21:56.036915Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, enc_vocab_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(enc_vocab_size, d_model, max_len, drop_prob)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(\n",
    "            d_model=d_model,\n",
    "            n_hidden=ffn_hidden,\n",
    "            n_head=n_head,\n",
    "            drop_prob=drop_prob\n",
    "        ) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x, src_mask):   # (batch_size, max_len)\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b767a2c49ffd5a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.049870Z",
     "start_time": "2024-08-24T09:21:56.043004Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, n_head, drop_prob):\n",
    "        super().__init__()\n",
    "        # sublayer 1\n",
    "        self.attention1 = MultiHeadAttention(d_model, n_head)   # self attention\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        # sublayer 2\n",
    "        self.attention2 = MultiHeadAttention(d_model, n_head)   # enc_dec_attention\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        # sublayer 3\n",
    "        self.ffn = PositionWiseFeedForward(d_model, n_hidden, drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, dec, enc, trg_mask, src_mask):\n",
    "        # dec is the previous output  (batch_size, max_length, d_model)\n",
    "        _x = dec\n",
    "        x = self.attention1(dec, dec, dec, mask=trg_mask)   # don't get information of following tokens  -> tril mat\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "            x = self.attention2(q=x, k=enc, v=enc, mask=src_mask)   # (batch_size, max_length, d_model)    -> change <pad> -> negative infinity\n",
    "            \n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "        \n",
    "        _x = x  \n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x    # (batch_size, max_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d39da638b685e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.055785Z",
     "start_time": "2024-08-24T09:21:56.050878Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_vocab_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, drop_prob=drop_prob, max_len=max_len, vocab_size=dec_vocab_size)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, ffn_hidden, n_head, drop_prob) for _ in range(n_layers)])\n",
    "        self.linear = nn.Linear(d_model, dec_vocab_size)\n",
    "        \n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "            \n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe9b9c36afaf46d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:44:17.647169Z",
     "start_time": "2024-08-24T09:44:17.640696Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_bos_idx, enc_voc_size, dev_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_vocab_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers)\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_vocab_size=dev_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers)\n",
    "        \n",
    "    def forward(self, src, trg):    \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(src)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2).to(torch.long)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(\"cuda\")\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96c5f00e8cf87dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T11:45:25.932376Z",
     "start_time": "2024-08-24T11:45:25.922927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 7])\n",
      "tensor([[-0.9139,  0.4950, -0.5827,  1.7843,  0.0912, -0.0686,  0.9386],\n",
      "        [-0.1362, -0.8931,  0.9399, -0.5369, -1.2713,  0.3504,  1.1994],\n",
      "        [-0.4657,  1.3153, -2.0890,  0.1006, -0.1378,  1.2424, -0.8644],\n",
      "        [ 0.5345,  0.4295, -1.4305, -0.6551,  0.6428,  0.4277,  1.8065],\n",
      "        [ 1.5145, -0.0970, -0.3496, -1.2437,  0.1786, -0.1402,  0.3212],\n",
      "        [-0.1933, -0.1760,  0.0326, -1.6319,  1.1695, -0.0429, -1.0467],\n",
      "        [ 0.5202, -0.9314, -0.7990,  1.8442, -0.7157,  1.7078,  1.0778]])\n",
      "tensor([[-9.1393e-01,  4.9496e-01, -5.8266e-01,  1.7843e+00, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [-1.3623e-01, -8.9307e-01,  9.3992e-01, -5.3694e-01, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [-4.6569e-01,  1.3153e+00, -2.0890e+00,  1.0064e-01, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [ 5.3445e-01,  4.2954e-01, -1.4305e+00, -6.5511e-01, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [ 1.5145e+00, -9.7021e-02, -3.4963e-01, -1.2437e+00, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [-1.9327e-01, -1.7599e-01,  3.2631e-02, -1.6319e+00, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04],\n",
      "        [ 5.2017e-01, -9.3138e-01, -7.9896e-01,  1.8442e+00, -1.0000e+04,\n",
      "         -1.0000e+04, -1.0000e+04]])\n",
      "tensor([[-0.3425, -1.0335,  0.5564,  0.2061,  1.1385,  0.2064,  0.1539],\n",
      "        [-0.0792, -0.3587,  0.5297, -0.7713,  1.9843, -0.9746,  0.2267],\n",
      "        [ 0.7884, -0.5240,  1.5573, -0.1735,  0.3487, -0.7023,  0.4033],\n",
      "        [ 0.6740, -0.9965,  0.6409, -0.9545,  0.5014, -0.4008,  0.5278],\n",
      "        [ 0.5984, -1.3772, -0.1428, -1.6712,  0.6525, -0.1911,  0.6417],\n",
      "        [ 0.3880, -0.5298,  0.7542, -0.8597,  1.2607, -0.8750,  0.3780],\n",
      "        [-0.4496, -1.3269,  0.0762, -0.1035,  1.1804,  0.4607,  0.2034]])\n",
      "tensor([[0.0469, 0.1917, 0.0653, 0.6961, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1971, 0.0925, 0.5783, 0.1321, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1124, 0.6673, 0.0222, 0.1981, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4265, 0.3840, 0.0598, 0.1298, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7052, 0.1407, 0.1093, 0.0447, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2851, 0.2900, 0.3573, 0.0676, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1901, 0.0445, 0.0508, 0.7145, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# internal mechanism of src_mask\n",
    "a = torch.tensor([[1, 2, 3, 4, 0, 0, 0],    # (2, 7)    -> (batch_size, max_len)\n",
    "                  [1, 9, 2, 0, 0, 0, 0]])\n",
    "src_mask = (a != 0).unsqueeze(1).unsqueeze(2).to(torch.long)   # (batch_size, 1, 1, max_len)   -> (2, 1, 1, 7)\n",
    "scores = torch.randn([2, 8, 7, 7])\n",
    "_scores = scores.masked_fill(src_mask==0, -10000)\n",
    "\n",
    "print(scores[0,0,:,:].shape)\n",
    "print(scores[0,0,:,:])\n",
    "print(_scores[0,0,:,:])\n",
    "v = torch.softmax(_scores, dim=-1) @ torch.randn([2, 8, 7, 7])\n",
    "print(v[0,0,:,:])\n",
    "print(torch.softmax(_scores, dim=-1)[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fd37b309658035b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T11:48:49.159514Z",
     "start_time": "2024-08-24T11:48:49.154928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mask = src_mask[0, 0, :, :]\n",
    "_mask.transpose(0, 1) * _mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0ab81744dd153c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:21:56.130454Z",
     "start_time": "2024-08-24T09:21:56.122239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0]])\n",
      "----------------------------------------\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "my_mask = src_mask * src_mask.transpose(2, 3)\n",
    "print(torch.cat([src_mask[0,0,:,:] for _ in range(7)]))\n",
    "print('-'*40)\n",
    "print(my_mask[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c02a011e8359913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:50:08.571150Z",
     "start_time": "2024-08-24T09:50:08.562881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0]]]])\n",
      "tensor([[-1.0952, -0.2178, -0.2489, -0.8556, -1.7917,  1.8808,  0.2819],\n",
      "        [-0.9901, -0.3119, -0.0455,  0.3520, -0.1477,  0.0234, -0.7999],\n",
      "        [-0.6403, -1.4774,  1.0258, -1.2600, -1.3721, -1.6903, -0.1964],\n",
      "        [-0.4174, -0.0520,  1.7719,  0.2723,  1.7473,  0.2218, -0.0814],\n",
      "        [ 1.6706,  0.8964, -0.5824,  0.1054,  0.2975, -0.8554, -1.0972],\n",
      "        [-0.3924, -2.1945, -0.3546, -1.6486,  0.4085, -0.0718,  0.4826],\n",
      "        [-0.2904,  0.5262, -0.8305,  0.0751,  2.2724,  1.4320,  0.1211]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3367, 0.6633, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1487, 0.0644, 0.7869, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0748, 0.1078, 0.6682, 0.1491, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])\n"
     ]
    }
   ],
   "source": [
    "trg = torch.tensor([1, 3, 8, 2, 0, 0, 0]).view(1, -1)   # (1, 7)    1 -> sos ; 2-> eos; 0->pad  (batch_size, max_len)\n",
    "max_len = trg.shape[1]\n",
    "trg_pad_mask = (trg != 0).unsqueeze(1).unsqueeze(3)     # (batch_size, 1, max_len, 1)\n",
    "trg_sub_mask = torch.tril(torch.ones(max_len, max_len)).to(torch.long) # (max_len, max_len) \n",
    "trg_mask = trg_pad_mask & trg_sub_mask  # (batch_size, 1, 1, max_len)\n",
    "print(trg_mask)\n",
    "\n",
    "scores = torch.randn([1, 8, 7, 7])\n",
    "_scores = scores.masked_fill(trg_mask==0, -10000)\n",
    "\n",
    "print(scores[0,0,:,:])\n",
    "print(torch.softmax(_scores[0,0,:,:], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee2b1c73cf4466e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T11:53:22.047683Z",
     "start_time": "2024-08-24T11:53:22.041453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(max_len, max_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
