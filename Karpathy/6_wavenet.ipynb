{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:45.996128Z",
     "start_time": "2024-07-19T12:18:42.677081Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.005140Z",
     "start_time": "2024-07-19T12:18:45.997128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read words\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "# Build vocabulary of all characters and mapping to/from integer\n",
    "chars = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(chars) + 1"
   ],
   "id": "a3e94c3847453aa1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.479274Z",
     "start_time": "2024-07-19T12:18:46.005140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ],
   "id": "b31ac263b915ea3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182778, 8]) torch.Size([182778])\n",
      "torch.Size([22633, 8]) torch.Size([22633])\n",
      "torch.Size([22735, 8]) torch.Size([22735])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.488501Z",
     "start_time": "2024-07-19T12:18:46.480273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Near copy paste of the layers we have developed in Part 3\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ],
   "id": "cae013e8c50ec9a8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.525717Z",
     "start_time": "2024-07-19T12:18:46.490498Z"
    }
   },
   "cell_type": "code",
   "source": "torch.manual_seed(42)",
   "id": "7a7df99836c0dbcf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x281039cd930>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.532272Z",
     "start_time": "2024-07-19T12:18:46.526718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# original network\n",
    "# n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "# n_hidden = 300 # the number of neurons in the hidden layer of the MLP\n",
    "# model = Sequential([\n",
    "#   Embedding(vocab_size, n_embd),\n",
    "#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "# hierarchical network\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# parameter init\n",
    "#with torch.no_grad():\n",
    "  #model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total"
   ],
   "id": "dc5159170edac7e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.535840Z",
     "start_time": "2024-07-19T12:18:46.533273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "id": "5c1862056b2011c5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:46.551140Z",
     "start_time": "2024-07-19T12:18:46.535840Z"
    }
   },
   "cell_type": "code",
   "source": "parameters",
   "id": "ad8e5b8e1ef6e6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01,\n",
       "          -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00,\n",
       "          -3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
       "           7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01,\n",
       "          -7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00],\n",
       "         [ 1.2791e+00,  1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,\n",
       "           4.1759e-02, -2.5158e-01,  8.5986e-01, -1.3847e+00, -8.7124e-01,\n",
       "          -2.2337e-01,  1.7174e+00,  3.1888e-01, -4.2452e-01,  3.0572e-01,\n",
       "          -7.7459e-01, -1.5576e+00,  9.9564e-01, -8.7979e-01, -6.0114e-01,\n",
       "          -1.2742e+00,  2.1228e+00, -1.2347e+00, -4.8791e-01],\n",
       "         [-9.1382e-01, -6.5814e-01,  7.8024e-02,  5.2581e-01, -4.8799e-01,\n",
       "           1.1914e+00, -8.1401e-01, -7.3599e-01, -1.4032e+00,  3.6004e-02,\n",
       "          -6.3477e-02,  6.7561e-01, -9.7807e-02,  1.8446e+00, -1.1845e+00,\n",
       "           1.3835e+00,  1.4451e+00,  8.5641e-01,  2.2181e+00,  5.2317e-01,\n",
       "           3.4665e-01, -1.9733e-01, -1.0546e+00,  1.2780e+00],\n",
       "         [-1.7219e-01,  5.2379e-01,  5.6622e-02,  4.2630e-01,  5.7501e-01,\n",
       "          -6.4172e-01, -2.2064e+00, -7.5080e-01,  1.0868e-02, -3.3874e-01,\n",
       "          -1.3407e+00, -5.8537e-01,  5.3619e-01,  5.2462e-01,  1.1412e+00,\n",
       "           5.1644e-02,  7.4395e-01, -4.8158e-01, -1.0495e+00,  6.0390e-01,\n",
       "          -1.7223e+00, -8.2777e-01,  1.3347e+00,  4.8354e-01],\n",
       "         [-2.5095e+00,  4.8800e-01,  7.8459e-01,  2.8647e-02,  6.4076e-01,\n",
       "           5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,  7.5276e-01,\n",
       "           4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00, -1.3109e-03,\n",
       "          -3.0360e-01, -1.4570e+00, -1.0234e-01, -5.9915e-01,  4.7706e-01,\n",
       "           7.2618e-01,  9.1152e-02, -3.8907e-01,  5.2792e-01],\n",
       "         [-1.2685e-02,  2.4084e-01,  1.3254e-01,  7.6424e-01,  1.0950e+00,\n",
       "           3.3989e-01,  7.1997e-01,  4.1141e-01,  1.9312e+00,  1.0119e+00,\n",
       "          -1.4364e+00, -1.1299e+00, -1.3603e-01,  1.6354e+00,  6.5474e-01,\n",
       "           5.7600e-01,  1.1415e+00,  1.8565e-02, -1.8058e+00,  9.2543e-01,\n",
       "          -3.7534e-01,  1.0331e+00, -6.8665e-01,  6.3681e-01],\n",
       "         [-9.7267e-01,  9.5846e-01,  1.6192e+00,  1.4506e+00,  2.6948e-01,\n",
       "          -2.1038e-01, -7.3280e-01,  1.0430e-01,  3.4875e-01,  9.6759e-01,\n",
       "          -4.6569e-01,  1.6048e+00, -2.4801e+00, -4.1754e-01, -1.1955e+00,\n",
       "           8.1234e-01, -1.9006e+00,  2.2858e-01,  2.4859e-02, -3.4595e-01,\n",
       "           2.8683e-01, -7.3084e-01,  1.7482e-01, -1.0939e+00],\n",
       "         [-1.6022e+00,  1.3529e+00,  1.2888e+00,  5.2295e-02, -1.5469e+00,\n",
       "           7.5671e-01,  7.7552e-01,  2.0265e+00,  3.5818e-02,  1.2059e-01,\n",
       "          -8.0566e-01, -2.0758e-01, -9.3195e-01, -1.5910e+00, -1.1360e+00,\n",
       "          -5.2260e-01, -5.1877e-01, -1.5013e+00, -1.9267e+00,  1.2785e-01,\n",
       "           1.0229e+00, -5.5580e-01,  7.0427e-01,  7.0988e-01],\n",
       "         [ 1.7744e+00, -9.2155e-01,  9.6245e-01, -3.3702e-01, -1.1753e+00,\n",
       "           3.5806e-01,  4.7877e-01,  1.3537e+00,  5.2606e-01,  2.1120e+00,\n",
       "          -5.2076e-01, -9.3201e-01,  1.8516e-01,  1.0687e+00,  1.3065e+00,\n",
       "           4.5983e-01, -8.1463e-01, -1.0212e+00, -4.9492e-01, -5.9225e-01,\n",
       "           1.5432e-01,  4.4077e-01, -1.4829e-01, -2.3184e+00],\n",
       "         [-3.9800e-01,  1.0805e+00, -1.7809e+00,  1.5080e+00,  3.0943e-01,\n",
       "          -5.0031e-01,  1.0350e+00,  1.6896e+00, -4.5051e-03,  1.6668e+00,\n",
       "           1.5392e-01, -1.0603e+00, -5.7266e-01,  8.3568e-02,  3.9991e-01,\n",
       "           1.9892e+00, -7.1988e-02, -9.0609e-01, -2.0487e+00, -1.0811e+00,\n",
       "           1.7623e-02,  7.8226e-02,  1.9316e-01,  4.0967e-01],\n",
       "         [-9.2913e-01,  2.7619e-01, -5.3888e-01,  4.6258e-01, -8.7189e-01,\n",
       "          -2.7118e-02, -3.5325e-01,  1.4639e+00,  1.2554e+00, -7.1496e-01,\n",
       "           8.5392e-01,  5.1299e-01,  5.3973e-01,  5.6551e-01,  5.0579e-01,\n",
       "           2.2245e-01, -6.8548e-01,  5.6356e-01, -1.5072e+00, -1.6107e+00,\n",
       "          -1.4790e+00,  4.3227e-01, -1.2503e-01,  7.8212e-01],\n",
       "         [-1.5988e+00, -1.0913e-01,  7.1520e-01,  3.9139e-02,  1.3059e+00,\n",
       "           2.4659e-01, -1.9776e+00,  1.7896e-02, -1.3793e+00,  6.2580e-01,\n",
       "          -2.5850e+00, -2.4000e-02, -1.2219e-01, -7.4700e-01,  1.7093e+00,\n",
       "           5.7923e-02,  1.1930e+00,  1.9373e+00,  7.2871e-01,  9.8089e-01,\n",
       "           4.1459e-01,  1.1566e+00,  2.6905e-01, -3.6629e-02],\n",
       "         [ 9.7329e-01, -1.0151e+00, -5.4192e-01, -4.4102e-01, -3.1362e-01,\n",
       "          -1.2925e-01, -7.1496e-01, -4.7562e-02,  2.0207e+00,  2.5392e-01,\n",
       "           9.3644e-01,  7.1224e-01, -3.1766e-02,  1.0164e-01,  1.3433e+00,\n",
       "           7.1327e-01,  4.0380e-01, -7.1398e-01,  8.3373e-01, -9.5855e-01,\n",
       "           4.5363e-01,  1.2461e+00, -2.3065e+00, -1.2869e+00],\n",
       "         [ 1.7989e-01, -2.1268e+00, -1.3408e-01, -1.0408e+00, -7.6472e-01,\n",
       "          -5.5283e-02,  1.2049e+00, -9.8247e-01,  4.3344e-01, -7.1719e-01,\n",
       "           1.0554e+00, -1.4534e+00,  4.6515e-01,  3.7139e-01, -4.6568e-03,\n",
       "           7.9549e-02,  3.7818e-01,  7.0511e-01, -1.7237e+00, -8.4348e-01,\n",
       "           4.3514e-01,  2.6589e-01, -5.8710e-01,  8.2689e-02],\n",
       "         [ 8.8538e-01,  1.8244e-01,  7.8638e-01, -5.7920e-02,  5.6667e-01,\n",
       "          -7.0976e-01, -4.8751e-01,  5.0096e-02,  6.0841e-01,  1.6309e+00,\n",
       "          -8.4723e-02,  1.0844e+00,  9.4777e-01, -6.7663e-01, -5.7302e-01,\n",
       "          -3.3032e-01, -7.9394e-01,  3.7523e-01,  8.7910e-02, -1.2415e+00,\n",
       "          -3.2025e-01, -8.4438e-01, -5.5135e-01,  1.9890e+00],\n",
       "         [ 1.9003e+00,  1.6951e+00,  2.8090e-02, -1.7537e-01, -1.7735e+00,\n",
       "          -7.0464e-01, -3.9465e-01,  1.8868e+00, -2.1844e-01,  1.6630e-01,\n",
       "           2.1442e+00,  1.7046e+00,  3.4590e-01,  6.4248e-01, -2.0395e-01,\n",
       "           6.8537e-01, -1.3969e-01, -1.1808e+00, -1.2829e+00,  4.4849e-01,\n",
       "          -5.9074e-01,  8.5406e-01, -4.9007e-01, -3.5946e-01],\n",
       "         [ 6.6637e-01, -7.4265e-02, -2.0960e-01,  1.6632e-01,  1.4703e+00,\n",
       "          -9.3909e-01, -6.0132e-01, -9.9640e-02, -9.8515e-01, -2.4885e+00,\n",
       "          -3.3132e-01,  8.4358e-01,  9.8745e-01, -3.3197e-01, -8.0762e-01,\n",
       "           8.2436e-01,  2.4700e-02, -1.0641e+00, -7.6019e-01, -4.0751e-01,\n",
       "           9.6236e-01, -1.4264e-01,  1.5271e-01, -3.8802e-02],\n",
       "         [ 9.4461e-01, -1.5824e+00,  9.8713e-01,  1.1457e+00, -1.4181e-01,\n",
       "          -2.7634e-01, -1.9321e-01,  7.7678e-01,  6.8388e-01, -1.3246e+00,\n",
       "          -5.1608e-01,  6.0018e-01, -4.7022e-01, -6.0864e-01, -4.6192e-02,\n",
       "          -1.6457e+00, -4.8333e-01, -7.4029e-01,  3.1428e-01,  1.4156e-01,\n",
       "           1.0348e+00, -6.2644e-01, -5.1509e-01,  6.9029e-01],\n",
       "         [-4.9400e-01,  1.1366e+00, -4.6184e-01,  1.4200e+00,  8.4852e-01,\n",
       "          -4.7891e-02,  6.6856e-01,  1.0430e+00,  6.8990e-01, -1.3129e+00,\n",
       "           3.7804e-02, -1.1702e+00, -1.0319e-01,  1.1895e+00,  7.6069e-01,\n",
       "          -7.4630e-01, -1.3839e+00,  4.8687e-01, -1.0020e+00,  3.2949e-02,\n",
       "          -4.2920e-01, -9.8180e-01, -6.4206e-01,  8.2659e-01],\n",
       "         [ 1.5914e+00, -1.2081e-01, -4.8302e-01,  1.1330e-01,  7.7151e-02,\n",
       "          -9.2281e-01, -1.2620e+00,  1.0861e+00,  1.0966e+00, -6.8369e-01,\n",
       "           6.6043e-02, -7.7380e-04,  1.6206e-01,  1.1960e+00, -1.3062e+00,\n",
       "          -1.4040e+00, -1.0597e+00,  3.0573e-01,  4.1506e-01, -7.1741e-01,\n",
       "           2.8340e+00,  1.9535e+00,  2.0487e+00, -1.0880e+00],\n",
       "         [ 1.6217e+00,  8.5127e-01, -4.0047e-01, -6.0883e-01, -5.0810e-01,\n",
       "          -6.1849e-01, -1.6470e+00, -1.0362e+00, -4.5031e-01, -7.2966e-02,\n",
       "          -5.4795e-01, -1.1426e+00, -4.4875e-01, -3.0454e-02,  3.8303e-01,\n",
       "          -4.4770e-02,  1.1799e+00, -3.3143e-01,  6.4950e-01,  9.4959e-02,\n",
       "          -7.5259e-01, -6.4723e-01, -1.2823e+00,  1.9653e+00],\n",
       "         [-9.6385e-01, -2.5668e+00,  7.0961e-01,  8.1984e-01,  6.2145e-01,\n",
       "           4.2319e-01, -3.3890e-01,  5.1797e-01, -1.3638e+00,  1.9296e-01,\n",
       "          -6.1033e-01,  1.6323e-01,  1.5102e+00,  2.1230e-01, -7.2520e-01,\n",
       "          -9.5277e-01,  5.2169e-01, -4.6387e-01,  1.8238e-01, -3.8666e-01,\n",
       "          -1.7907e+00,  9.3293e-02, -1.9153e+00, -6.4218e-01],\n",
       "         [ 1.3439e+00, -1.2922e+00,  7.6624e-01,  6.4540e-01,  3.5332e-01,\n",
       "          -2.6475e+00, -1.4575e+00, -9.7124e-01,  2.5403e-01, -1.7906e-01,\n",
       "           1.1993e+00, -4.2922e-01,  1.0103e+00,  6.1104e-01,  1.2208e+00,\n",
       "          -6.0764e-01, -1.7376e+00, -1.2535e-01, -1.3658e+00,  1.1117e+00,\n",
       "          -6.2280e-01, -7.8918e-01, -1.6782e-01,  1.6433e+00],\n",
       "         [ 2.0071e+00, -1.2531e+00,  1.1189e+00,  1.7733e+00, -2.0717e+00,\n",
       "          -4.1253e-01, -9.7696e-01, -3.3634e-02,  1.8595e+00,  2.6221e+00,\n",
       "           3.6905e-01,  3.8030e-01,  1.9898e-01, -2.3609e-01,  3.0341e-01,\n",
       "          -4.5008e-01,  4.7390e-01,  6.5034e-01,  1.1662e+00,  1.6936e-02,\n",
       "           5.3259e-01, -6.0354e-01, -1.7426e-01,  6.0921e-01],\n",
       "         [-8.0322e-01, -1.1209e+00,  1.9564e-01, -7.8152e-01, -1.7899e+00,\n",
       "          -2.6157e-01, -4.4025e-01,  2.1848e+00, -4.8010e-01, -1.2872e+00,\n",
       "           7.3888e-01,  3.3895e-02, -3.1229e-01, -2.5418e-01, -1.2055e+00,\n",
       "          -9.5421e-01,  6.1277e-02,  8.5261e-02,  7.4813e-01, -1.6356e-01,\n",
       "          -9.0856e-01,  3.1300e-01,  8.0505e-01, -1.1134e+00],\n",
       "         [ 4.9816e-01, -1.2000e+00,  1.2711e-01,  4.4037e-01,  6.3777e-01,\n",
       "           1.5979e-01,  1.7698e+00,  6.2682e-01, -1.8737e+00,  2.3259e+00,\n",
       "          -9.2039e-01,  6.6611e-01, -4.4026e-01, -2.3180e+00,  1.2946e+00,\n",
       "           2.2267e-01, -8.4834e-01,  1.6489e+00,  1.6006e+00, -7.8589e-02,\n",
       "           4.3105e-01,  3.6835e-01,  7.6380e-01,  1.1792e+00],\n",
       "         [-4.1379e-01,  5.1841e-01, -7.0154e-01, -4.3234e-01,  1.4148e-01,\n",
       "           7.1104e-02,  5.6335e-01, -5.7864e-01, -9.4374e-01,  1.7305e-01,\n",
       "          -1.8815e+00,  5.8509e-01,  1.5287e+00, -9.3240e-01,  1.3527e+00,\n",
       "           1.6028e-01,  5.3738e-01,  7.8175e-01,  1.0477e+00, -3.9481e-01,\n",
       "           1.6077e+00, -8.0643e-01,  7.3201e-02, -2.0952e+00]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 0.2193,  0.0052,  0.1496,  ..., -0.0139,  0.2013, -0.1877],\n",
       "         [-0.1952, -0.0188,  0.2533,  ..., -0.0944,  0.0349,  0.1995],\n",
       "         [-0.0248, -0.1973,  0.0938,  ...,  0.1047, -0.0850, -0.1437],\n",
       "         ...,\n",
       "         [-0.1164, -0.2138, -0.2090,  ...,  0.0727,  0.1812, -0.0981],\n",
       "         [-0.2983, -0.0168,  0.0894,  ..., -0.0240,  0.1211,  0.1981],\n",
       "         [-0.0894,  0.0122, -0.0715,  ...,  0.0452, -0.1401, -0.0129]],\n",
       "        requires_grad=True),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.], requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " tensor([[ 0.0343,  0.0071, -0.0357,  ...,  0.0353, -0.0215, -0.0500],\n",
       "         [ 0.0518,  0.0974,  0.0533,  ..., -0.0409,  0.0462,  0.0280],\n",
       "         [-0.0428, -0.0553,  0.0870,  ..., -0.0410,  0.0481,  0.0540],\n",
       "         ...,\n",
       "         [-0.0400, -0.0743,  0.0558,  ...,  0.0203,  0.0010,  0.0037],\n",
       "         [ 0.0173, -0.0349,  0.0516,  ...,  0.0768, -0.0369,  0.0485],\n",
       "         [-0.1264, -0.0124, -0.0524,  ..., -0.0989, -0.0025,  0.0147]],\n",
       "        requires_grad=True),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.], requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " tensor([[-0.0281, -0.0274, -0.0743,  ...,  0.0874, -0.0369, -0.0249],\n",
       "         [ 0.0720,  0.0153, -0.0347,  ...,  0.0176, -0.0393, -0.0717],\n",
       "         [ 0.0137, -0.0069,  0.0604,  ..., -0.1355,  0.0519, -0.0334],\n",
       "         ...,\n",
       "         [ 0.0862,  0.0085,  0.0396,  ..., -0.0296,  0.1007,  0.0088],\n",
       "         [-0.0258,  0.0829,  0.0954,  ...,  0.0763,  0.0875, -0.1466],\n",
       "         [ 0.0730,  0.0380, -0.0193,  ...,  0.0172, -0.1212,  0.0386]],\n",
       "        requires_grad=True),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.], requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " tensor([[ 0.0944,  0.0279, -0.0359,  ..., -0.0219,  0.0396, -0.0269],\n",
       "         [ 0.0821,  0.0548, -0.0564,  ..., -0.0051, -0.0387,  0.0321],\n",
       "         [-0.0307, -0.0936, -0.0537,  ...,  0.0561,  0.0196, -0.1176],\n",
       "         ...,\n",
       "         [ 0.0209, -0.0731, -0.0679,  ..., -0.0697, -0.1335,  0.0669],\n",
       "         [-0.0358,  0.0220,  0.0725,  ..., -0.1644,  0.0643,  0.1275],\n",
       "         [-0.0241, -0.0379,  0.1355,  ...,  0.1168,  0.0940, -0.0208]],\n",
       "        requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:18:58.187525Z",
     "start_time": "2024-07-19T12:18:46.552139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  logits = model(Xb)\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ],
   "id": "3ad07113d3406300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.4213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m Xb, Yb \u001B[38;5;241m=\u001B[39m Xtr[ix], Ytr[ix] \u001B[38;5;66;03m# batch X,Y\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m logits \u001B[38;5;241m=\u001B[39m model(Xb)\n\u001B[0;32m     14\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, Yb) \u001B[38;5;66;03m# loss function\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# backward pass\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 103\u001B[0m, in \u001B[0;36mSequential.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    102\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 103\u001B[0m     x \u001B[38;5;241m=\u001B[39m layer(x)\n\u001B[0;32m    104\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m    105\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout\n",
      "Cell \u001B[1;32mIn[4], line 60\u001B[0m, in \u001B[0;36mTanh.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 60\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(x)\n\u001B[0;32m     61\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bb99390d03ae0008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac6ac105fd43ba0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "435066eb5648f48a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9d9f84c9dcda957d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "49cf49b25f61e5c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
